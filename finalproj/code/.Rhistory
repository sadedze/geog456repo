names(mtcars)
colnames(mtcars)
mtcars [1,]
mtcars [,1]
mtcars [1,1]
mtcars[[1]]
getwd
getwd()
setwd(C:/Users/Senam Adedze/OneDrive - University of North Carolina at Chapel Hill/SP2024/LING460/data)
setwd(C:/Users/Senam Adedze/OneDrive - University of North Carolina at Chapel Hill/SP2024/LING460/data)
binom.test(x=18, n=60, p=1/2)
libray(lititanic)
libray(Titanic)
library(Titanic)
attach(Titanic)
install.packages(datasets)
install.packages("datasets")
install.packages("datasets")
library(titanic)
library(Titanic)
?Titanic
attach(Titanic)
class(Titanic)
as.data.frame(Titanic)
tfa23 <- subset(
as.data.frame(Titanic),
Sex=="Female" & Age=="Adult" & (Class=="2nd" | Class=="3rd"))
table(tfa23, Class=="2nd")
View(tfa23)
View(tfa23)
install.packages("psych")
install.packages("glmnet")
library (tidytext)
library (tm)
library (textdata)
library (psych) # For Cohen's kappa
library (glmnet) # For the LASSO regression model
imdb_jr <- read_csv("data/imdb_jr.csv")
library(tidyverse)
imdb_jr <- read_csv("data/imdb_jr.csv")
setwd('C:/Users/Senam Adedze/OneDrive - University of North Carolina at Chapel Hill/SP2024/LING460')
imdb_jr <- read_csv("data/imdb_jr.csv")
imdb_jr_corpus <- Corpus (VectorSource (imdb_jr$text))
imdb_jr_dtm_all <- DocumentTermMatrix (
imdb_jr_corpus,
control = cleaning.profile
)
cleaning.profile <- list (
stripWhitespace=TRUE,
removeNumbers=TRUE,
removePunctuation=TRUE,
tolower=TRUE,
stopwords=TRUE,
stemming=TRUE,
weighting = weightBin # or weightTf or weightTfIdf
)
imdb_jr_dtm_all <- DocumentTermMatrix (
imdb_jr_corpus,
control = cleaning.profile
)
imdb_jr_mat <- as.matrix (imdb_jr_dtm)
imdb_jr_dtm <- removeSparseTerms (imdb_jr_dtm_all, sparse=0.95
imdb_jr_dtm <- removeSparseTerms (imdb_jr_dtm_all, sparse=0.95)
imdb_jr_mat <- as.matrix (imdb_jr_dtm)
training <- which (
1:nrow (imdb_jr) %% 4 > 0 # if divisible by 4 print true (equal 1)
)
testing <- which (
1:nrow (imdb_jr) %% 4 == 0 # if not div by 4 print false (equal 0)
)
training_set <- imdb_jr_mat [training,]
testing_set <- imdb_jr_mat [testing,]
storage.mode (imdb_jr_mat) <- "logical"
imdb_jr_dtm <- removeSparseTerms (imdb_jr_dtm_all, sparse=0.95)
imdb_jr_mat <- as.matrix (imdb_jr_dtm)
imdb_jr <- read_csv("data/imdb_jr.csv")
imdb_jr_corpus <- Corpus (VectorSource (imdb_jr$text))
# clean corpus
cleaning.profile <- list (
stripWhitespace=TRUE,
removeNumbers=TRUE,
removePunctuation=TRUE,
tolower=TRUE,
stopwords=TRUE,
stemming=TRUE,
weighting = weightBin # or weightTf or weightTfIdf
)
imdb_jr_dtm_all <- DocumentTermMatrix (
imdb_jr_corpus,
control = cleaning.profile
)
# remove sparse terms
imdb_jr_dtm <- removeSparseTerms (imdb_jr_dtm_all, sparse=0.95)
imdb_jr_mat <- as.matrix (imdb_jr_dtm)
# create testing and training set
training <- which (
1:nrow (imdb_jr) %% 4 > 0 # if divisible by 4 print true (equal 1)
)
testing <- which (
1:nrow (imdb_jr) %% 4 == 0 # if not div by 4 print false (equal 0)
)
training_set <- imdb_jr_mat [training,]
testing_set <- imdb_jr_mat [testing,]
storage.mode (imdb_jr_mat) <- "logical"
# Need to insure equal numbers of positive and negative review
training <- which (1:nrow (imdb_jr) %% 4 > 0)
training_set <- imdb_jr_mat [training,]
training_labels <- imdb_jr$sentiment [training]
testing <- which (1:nrow (imdb_jr) %% 4 == 0)
testing_set <- imdb_jr_mat [testing,]
testing_labels <- imdb_jr$sentiment [testing]
df <- data_frame (
labels = (training_labels == "pos")
)
df <- cbind (df, training_set)
f <- as.formula (
paste ("labels ~ ",
paste (colnames (training_set),
collapse = "+"))
)
imdb_jr_model <- glm (formula = f, data=df, family=binomial
imdb_jr_model <- glm (formula = f, data=df, family=binomial)
imdb_jr_model <- glm (formula = f, data=df, family=binomial)
df <- data_frame (
labels = (training_labels == "pos")
)
df <- cbind (df, training_set)
f <- as.formula (
paste ("labels ~ ",
paste (colnames (training_set),
collapse = "+"))
)
imdb_jr_model <- glm (formula = f, data=df, family=binomial)
df <- data_frame (
labels = (training_labels == "pos")
)
df <- cbind (df, training_set)
f <- as.formula (
paste ("labels ~ `",
paste (colnames (training_set),
collapse = "\`+\`", sep = ""), "`", sep = "")
)
imdb_jr_model <- glm (formula = f, data=df, family=binomial)
summary(imdb_jr_model)
imdb_jr_model <- cv.glmnet(
training_set,
training_labels,
family = "binomial",
type.measure = "class"
)
imdb_jr_model <- glm (formula = f, data=df, family=binomial)
imdb_jr_model_lasso <- cv.glmnet(
training_set,
training_labels,
family = "binomial",
type.measure = "class"
)
plot (imdb_jr_model)
plot (imdb_jr_model_lasso)
coef(imdb_jr_model, s = "lambda.min")
coef(imdb_jr_model_lasso, s = "lambda.min")
words <- rownames (coef(imdb_jr_model, s = "lambda.min"))
weights <- as.vector (coef(imdb_jr_model, s = "lambda.min"))
data.frame (
words = words [order (weights)],
weights = sort (weights)
) |> filter (weights !=0)
words <- rownames (coef(imdb_jr_model_lasso, s = "lambda.min"))
weights <- as.vector (coef(imdb_jr_model_lasso, s = "lambda.min"))
data.frame (
words = words [order (weights)],
weights = sort (weights)
) |> filter (weights !=0)
summary(imdb_jr_model_lasso)
imdb_jr_nb_pred <- as.character (predict (
imdb_jr_nb, testing_set,
type = "class" # Use MAP decision rule
))
imdb_jr_lasso_pred <- as.character (predict (
imdb_jr_model_lasso, testing_set,
type = "class" # Use MAP decision rule
))
imdb_jr_lasso_pred [1:10]
cleaning.profile <- list (
stripWhitespace=TRUE,
removeNumbers=TRUE,
removePunctuation=TRUE,
tolower=TRUE,
stopwords=TRUE,
stemming=TRUE,
weighting = weightTf #weightBin  or weightTfIdf
)
imdb_jr_dtm_all <- DocumentTermMatrix (
imdb_jr_corpus,
control = cleaning.profile
)
# remove sparse terms
imdb_jr_dtm <- removeSparseTerms (imdb_jr_dtm_all, sparse=0.95)
imdb_jr_mat <- as.matrix (imdb_jr_dtm)
# create testing and training set
training <- which (
1:nrow (imdb_jr) %% 4 > 0 # if divisible by 4 print true (equal 1)
)
testing <- which (
1:nrow (imdb_jr) %% 4 == 0 # if not div by 4 print false (equal 0)
)
training_set <- imdb_jr_mat [training,]
testing_set <- imdb_jr_mat [testing,]
#storage.mode (imdb_jr_mat) <- "logical"
# Need to insure equal numbers of positive and negative review
training <- which (1:nrow (imdb_jr) %% 4 > 0)
training_set <- imdb_jr_mat [training,]
training_labels <- imdb_jr$sentiment [training]
testing <- which (1:nrow (imdb_jr) %% 4 == 0)
testing_set <- imdb_jr_mat [testing,]
testing_labels <- imdb_jr$sentiment [testing]
imdb_jr_model_lasso <- cv.glmnet(
training_set,
training_labels,
family = "binomial",
type.measure = "class"
)
plot (imdb_jr_model_lasso)
# check actual weights of which words matter
coef(imdb_jr_model_lasso, s = "lambda.min")
# sort from most negative and most positive
words <- rownames (coef(imdb_jr_model_lasso, s = "lambda.min"))
weights <- as.vector (coef(imdb_jr_model_lasso, s = "lambda.min"))
data.frame (
words = words [order (weights)],
weights = sort (weights)
) |> filter (weights !=0)
##############in class problem ##############
# make a prediction
imdb_jr_lasso_pred <- as.character (predict (
imdb_jr_model_lasso, testing_set,
type = "class" # Use MAP decision rule
))
# view
imdb_jr_lasso_pred [1:10]
cohen.kappa (cbind (imdb_jr_lasso_pred, testing_labels))
cleaning.profile <- list (
stripWhitespace=TRUE,
removeNumbers=TRUE,
removePunctuation=TRUE,
tolower=TRUE,
stopwords=TRUE,
stemming=TRUE,
weighting = weightTf #weightBin  or weightTfIdf
)
imdb_jr_dtm_all <- DocumentTermMatrix (
imdb_jr_corpus,
control = cleaning.profile
)
# remove sparse terms
imdb_jr_dtm <- removeSparseTerms (imdb_jr_dtm_all, sparse=0.9)
imdb_jr_mat <- as.matrix (imdb_jr_dtm)
# create testing and training set
training <- which (
1:nrow (imdb_jr) %% 4 > 0 # if divisible by 4 print true (equal 1)
)
testing <- which (
1:nrow (imdb_jr) %% 4 == 0 # if not div by 4 print false (equal 0)
)
training_set <- imdb_jr_mat [training,]
testing_set <- imdb_jr_mat [testing,]
#storage.mode (imdb_jr_mat) <- "logical"
# Need to insure equal numbers of positive and negative review
training <- which (1:nrow (imdb_jr) %% 4 > 0)
training_set <- imdb_jr_mat [training,]
training_labels <- imdb_jr$sentiment [training]
testing <- which (1:nrow (imdb_jr) %% 4 == 0)
testing_set <- imdb_jr_mat [testing,]
testing_labels <- imdb_jr$sentiment [testing]
imdb_jr_model_lasso <- cv.glmnet(
training_set,
training_labels,
family = "binomial",
type.measure = "class"
)
imdb_jr_lasso_pred <- as.character (predict (
imdb_jr_model_lasso, testing_set,
type = "class" # Use MAP decision rule
))
# view
imdb_jr_lasso_pred [1:10]
# see accuracy
cohen.kappa (cbind (imdb_jr_lasso_pred, testing_labels))
cleaning.profile <- list (
stripWhitespace=TRUE,
removeNumbers=TRUE,
removePunctuation=TRUE,
tolower=TRUE,
stopwords=TRUE,
stemming=TRUE,
weighting = weightTfIdf #weightTf #weightBin  or
)
imdb_jr_dtm_all <- DocumentTermMatrix (
imdb_jr_corpus,
control = cleaning.profile
)
# remove sparse terms
imdb_jr_dtm <- removeSparseTerms (imdb_jr_dtm_all, sparse=0.9)
imdb_jr_mat <- as.matrix (imdb_jr_dtm)
# create testing and training set
training <- which (
1:nrow (imdb_jr) %% 4 > 0 # if divisible by 4 print true (equal 1)
)
testing <- which (
1:nrow (imdb_jr) %% 4 == 0 # if not div by 4 print false (equal 0)
)
training_set <- imdb_jr_mat [training,]
testing_set <- imdb_jr_mat [testing,]
#storage.mode (imdb_jr_mat) <- "logical"
# Need to insure equal numbers of positive and negative review
training <- which (1:nrow (imdb_jr) %% 4 > 0)
training_set <- imdb_jr_mat [training,]
training_labels <- imdb_jr$sentiment [training]
testing <- which (1:nrow (imdb_jr) %% 4 == 0)
testing_set <- imdb_jr_mat [testing,]
testing_labels <- imdb_jr$sentiment [testing]
imdb_jr_model_lasso <- cv.glmnet(
training_set,
training_labels,
family = "binomial",
type.measure = "class"
)
# check actual weights of which words matter
coef(imdb_jr_model_lasso, s = "lambda.min")
imdb_jr_lasso_pred <- as.character (predict (
imdb_jr_model_lasso, testing_set,
type = "class" # Use MAP decision rule
))
cohen.kappa (cbind (imdb_jr_lasso_pred, testing_labels))
cleaning.profile <- list (
stripWhitespace=TRUE,
removeNumbers=TRUE,
removePunctuation=TRUE,
tolower=TRUE,
stopwords=TRUE,
stemming=TRUE,
weighting = weightTfIdf #weightTf #weightBin  or
)
imdb_jr_dtm_all <- DocumentTermMatrix (
imdb_jr_corpus,
control = cleaning.profile
)
# remove sparse terms
imdb_jr_dtm <- removeSparseTerms (imdb_jr_dtm_all, sparse=0.99)
imdb_jr_mat <- as.matrix (imdb_jr_dtm)
# create testing and training set
training <- which (
1:nrow (imdb_jr) %% 4 > 0 # if divisible by 4 print true (equal 1)
)
testing <- which (
1:nrow (imdb_jr) %% 4 == 0 # if not div by 4 print false (equal 0)
)
training_set <- imdb_jr_mat [training,]
testing_set <- imdb_jr_mat [testing,]
#storage.mode (imdb_jr_mat) <- "logical"
# Need to insure equal numbers of positive and negative review
training <- which (1:nrow (imdb_jr) %% 4 > 0)
training_set <- imdb_jr_mat [training,]
training_labels <- imdb_jr$sentiment [training]
testing <- which (1:nrow (imdb_jr) %% 4 == 0)
testing_set <- imdb_jr_mat [testing,]
testing_labels <- imdb_jr$sentiment [testing]
imdb_jr_model_lasso <- cv.glmnet(
training_set,
training_labels,
family = "binomial",
type.measure = "class"
)
imdb_jr_lasso_pred <- as.character (predict (
imdb_jr_model_lasso, testing_set,
type = "class" # Use MAP decision rule
))
cohen.kappa (cbind (imdb_jr_lasso_pred, testing_labels))
library(jsonlite)
df = read.csv("aggregated_closures_06052023.csv")
setwd("C:/Users/Senam Adedze/OneDrive - University of North Carolina at Chapel Hill/SP2024/GEOG456/New folder")
library(jsonlite)
df = read.csv("aggregated_closures_06052023.csv")
## make text into time
df$st = strptime(df$start, format = "%Y-%m-%dT%H:%M:%SZ")
df$fn = strptime(df$end, format = "%Y-%m-%dT%H:%M:%SZ")
myStart = strptime("2018-09-07T00:00:00Z", format = "%Y-%m-%dT%H:%M:%SZ")
myEnd = strptime("2018-09-30T00:00:00Z", format = "%Y-%m-%dT%H:%M:%SZ")
myNA = strptime("2019-09-30T00:00:00Z", format = "%Y-%m-%dT%H:%M:%SZ")
df2 = df[df$st >= myStart & df$st<= myEnd & df$fn <= myNA, ]
df2$t = (as.numeric(df2$st) - as.numeric(myStart))/(60*60*24)
df2$te = (as.numeric(df2$fn) - as.numeric(myStart))/(60*60*24)
# Convert data to JSON
json_data <- toJSON(df2)
# Write JSON to file
write(json_data, "closures.json")
View(df2)
install.packages("languageserver")
summary(df2$te)
View(df2)
summary(df2$t)
View(df2)
library(janeaustenr)
view(tidy_books)
library(tidytext)
library(janeaustenr)
library(dplyr)
library(stringr)
library(tidyr)
jane_austen <- austen_books() %>%
unnest_tokens(bigram, text, token = "ngrams", n=2) %>%
filter(str_detect(bigram, '^[A-Z]')) %>% # start with capital letter
sum(str_detect(bigram, 'an$')) #num of bigrams that end with -an
jane_austen <- austen_books()
View(jane_austen)
jane_austen <- austen_books() %>%
unnest_tokens(bigram, text, token = "ngrams", n=2) %>%
filter(str_detect(bigram, '^[A-Z]')) %>% # start with capital letter
sum(str_detect(bigram, 'an$')) #num of bigrams that end with -an
jane_austen <- austen_books() %>%
unnest_tokens()
jane_austen <- austen_books() %>%
unnest_tokens(word, text)
View(jane_austen)
jane_austen <- austen_books() %>%
unnest_tokens(word, text) %>%
filter(str_detect('^[A-Z]'))
jane_austen <- austen_books() %>%
unnest_tokens(word, text, to_lower = FALSE) %>%
filter(str_detect(word, '^[A-Z]')) %>% # start with capital letter
View(jane_austen)
jane_austen <- austen_books() %>%
unnest_tokens(word, text, to_lower = FALSE) %>%
filter(str_detect(word, '^[A-Z]')) %>% # start with capital letter
filter(str_detect(word, 'an$')) %>% #num of bigrams that end with -an
count(word, sort = TRUE)
200*.4
8/88
jane_austen <- austen_books()
jane_austen <- austen_books() %>%
unnest_tokens(word, text, to_lower = FALSE)
View(jane_austen)
jane_austen <- austen_books() %>%
unnest_tokens(word, text, to_lower = FALSE) %>%
filter(str_detect(word, '^[A-Z]'))
View(jane_austen)
jane_austen <- austen_books() %>%
unnest_tokens(word, text, to_lower = FALSE) %>%
filter(str_detect(word, '^[A-Z]')) %>% # start with capital letter
filter(str_detect(word, 'an$'))
jane_austen <- austen_books() %>%
unnest_tokens(word, text, to_lower = FALSE) %>%
filter(str_detect(word, '^[A-Z]')) %>% # start with capital letter
filter(str_detect(word, 'an$')) %>% #num of bigrams that end with -an
count(word, sort = TRUE)
jane_austen <- austen_books() %>%
unnest_tokens(word, text, to_lower = FALSE) %>%
filter(str_detect(word, '^[A-Z]')) %>% # start with capital letter
filter(str_detect(word, 'an$')) %>% #num of bigrams that end with -an
summarise(count(word))
# problem 3
jane_austen <- austen_books() %>%arrange (-n) %>%
# problem 5
new 100
jane_austen <- austen_books() %>%
unnest_tokens(word, text) %>%
filter(str_detect(word, 'he|she'))
jane_austen <- austen_books() %>%
unnest_tokens(word, text, to_lower = FALSE) %>%
filter(str_detect(word, '^[A-Z]')) %>% # start with capital letter
filter(str_detect(word, 'an$')) %>% #num of bigrams that end with -an
summarise(count(word)) # come BACK and figure out how to count
# problem 2
jane_austen2 <- austen_books() %>%
unnest_tokens(bigram, text, token = "ngrams", n=2) %>%
count(word, sort = TRUE)
jane_austen3 <- austen_books() %>%
unnest_tokens(word, text) %>%
filter(str_detect(word, 'he|she'))
View(jane_austen3)
jane_austen3 <- austen_books() %>%
unnest_tokens(word, text) %>%
filter(str_detect(word, 'he|she'))
View(jane_austen3)
jane_austen3 <- austen_books() %>%
unnest_tokens(word, text) %>%
group_by(book) %>%
jane_austen3 <- austen_books() %>%
unnest_tokens(word, text) %>%
group_by(book) %>%
str_count(word,"^she$") %>%
View(jane_austen3)
setwd("C:/Users/Senam Adedze/OneDrive - University of North Carolina at Chapel Hill/SP2024/GEOG456/geog456repo/finalproj/code")
df <- read.csv("finalproj/data/aggregated_closures_06052023.csv")
df <- read.csv("C:/Users/Senam Adedze/OneDrive - University of North Carolina at Chapel Hill/SP2024/GEOG456/aggregated_closures_06052023.csv")
View(df)
library(jsonlite)
df <- read.csv("C:/Users/Senam Adedze/OneDrive - University of North Carolina at Chapel Hill/SP2024/GEOG456/aggregated_closures_06052023.csv")
## make text into time
df$st <- as.POSIXct(df$start, format = "%Y-%m-%dT%H:%M:%SZ")
df$fn <- as.POSIXct(df$end, format = "%Y-%m-%dT%H:%M:%SZ")
my_start <- as.POSIXct("2018-09-07T00:00:00Z", format = "%Y-%m-%dT%H:%M:%SZ")
my_end <- as.POSIXct("2018-09-30T00:00:00Z", format = "%Y-%m-%dT%H:%M:%SZ")
my_na <- as.POSIXct("2019-09-30T00:00:00Z", format = "%Y-%m-%dT%H:%M:%SZ")
df2 <- df[df$st >= my_start & df$st <= my_end & df$fn <= my_na, ]
df2$t <- as.numeric(difftime(df2$st, my_start, units = "days"))
df2$te <- as.numeric(difftime(df2$fn, my_start, units = "days"))
# Convert data to JSON
json_data <- toJSON(df2)
# Write JSON to file with .json extension
write(json_data, "finalproj/data.json")
View(df2)
